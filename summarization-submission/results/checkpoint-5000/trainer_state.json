{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 40.68556213378906,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 11.7473,
      "step": 10
    },
    {
      "epoch": 0.004,
      "grad_norm": 33.89143371582031,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 8.34,
      "step": 20
    },
    {
      "epoch": 0.006,
      "grad_norm": 18.29250717163086,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 8.6626,
      "step": 30
    },
    {
      "epoch": 0.008,
      "grad_norm": 40.93055725097656,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 7.333,
      "step": 40
    },
    {
      "epoch": 0.01,
      "grad_norm": 24.33718490600586,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 6.4154,
      "step": 50
    },
    {
      "epoch": 0.012,
      "grad_norm": 47.251914978027344,
      "learning_rate": 1.9920000000000002e-05,
      "loss": 5.4909,
      "step": 60
    },
    {
      "epoch": 0.014,
      "grad_norm": 29.012678146362305,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 5.2174,
      "step": 70
    },
    {
      "epoch": 0.016,
      "grad_norm": 33.39805221557617,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 3.3351,
      "step": 80
    },
    {
      "epoch": 0.018,
      "grad_norm": 38.66568374633789,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 3.9178,
      "step": 90
    },
    {
      "epoch": 0.02,
      "grad_norm": 50.725807189941406,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 3.9035,
      "step": 100
    },
    {
      "epoch": 0.022,
      "grad_norm": 12.376845359802246,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 3.3381,
      "step": 110
    },
    {
      "epoch": 0.024,
      "grad_norm": 9.611030578613281,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 2.7813,
      "step": 120
    },
    {
      "epoch": 0.026,
      "grad_norm": 7.379599094390869,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 2.7294,
      "step": 130
    },
    {
      "epoch": 0.028,
      "grad_norm": 5.162291526794434,
      "learning_rate": 1.9813333333333336e-05,
      "loss": 3.0483,
      "step": 140
    },
    {
      "epoch": 0.03,
      "grad_norm": 6.556812763214111,
      "learning_rate": 1.98e-05,
      "loss": 2.8902,
      "step": 150
    },
    {
      "epoch": 0.032,
      "grad_norm": 3.9914238452911377,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 2.7237,
      "step": 160
    },
    {
      "epoch": 0.034,
      "grad_norm": 6.792272567749023,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 2.2459,
      "step": 170
    },
    {
      "epoch": 0.036,
      "grad_norm": 43.73948287963867,
      "learning_rate": 1.976e-05,
      "loss": 3.1794,
      "step": 180
    },
    {
      "epoch": 0.038,
      "grad_norm": 7.4374237060546875,
      "learning_rate": 1.974666666666667e-05,
      "loss": 2.554,
      "step": 190
    },
    {
      "epoch": 0.04,
      "grad_norm": 10.205191612243652,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 1.8846,
      "step": 200
    },
    {
      "epoch": 0.042,
      "grad_norm": 15.236013412475586,
      "learning_rate": 1.972e-05,
      "loss": 2.8602,
      "step": 210
    },
    {
      "epoch": 0.044,
      "grad_norm": 4.725482940673828,
      "learning_rate": 1.970666666666667e-05,
      "loss": 2.3786,
      "step": 220
    },
    {
      "epoch": 0.046,
      "grad_norm": 4.673288822174072,
      "learning_rate": 1.9693333333333337e-05,
      "loss": 2.6222,
      "step": 230
    },
    {
      "epoch": 0.048,
      "grad_norm": 3.8453872203826904,
      "learning_rate": 1.968e-05,
      "loss": 2.3271,
      "step": 240
    },
    {
      "epoch": 0.05,
      "grad_norm": 4.060524940490723,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 2.2483,
      "step": 250
    },
    {
      "epoch": 0.052,
      "grad_norm": 4.313191890716553,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 1.9692,
      "step": 260
    },
    {
      "epoch": 0.054,
      "grad_norm": 4.566064834594727,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 2.1342,
      "step": 270
    },
    {
      "epoch": 0.056,
      "grad_norm": 7.426621913909912,
      "learning_rate": 1.9626666666666666e-05,
      "loss": 1.7607,
      "step": 280
    },
    {
      "epoch": 0.058,
      "grad_norm": 5.79258394241333,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 1.9409,
      "step": 290
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.679961919784546,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 2.2212,
      "step": 300
    },
    {
      "epoch": 0.062,
      "grad_norm": 5.2500104904174805,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 1.9544,
      "step": 310
    },
    {
      "epoch": 0.064,
      "grad_norm": 3.9217584133148193,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 2.3268,
      "step": 320
    },
    {
      "epoch": 0.066,
      "grad_norm": 2.9131510257720947,
      "learning_rate": 1.9560000000000002e-05,
      "loss": 2.2038,
      "step": 330
    },
    {
      "epoch": 0.068,
      "grad_norm": 3.0284461975097656,
      "learning_rate": 1.954666666666667e-05,
      "loss": 2.0444,
      "step": 340
    },
    {
      "epoch": 0.07,
      "grad_norm": 3.3292696475982666,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 2.1609,
      "step": 350
    },
    {
      "epoch": 0.072,
      "grad_norm": 2.940692901611328,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 1.9854,
      "step": 360
    },
    {
      "epoch": 0.074,
      "grad_norm": 3.26196026802063,
      "learning_rate": 1.950666666666667e-05,
      "loss": 1.5773,
      "step": 370
    },
    {
      "epoch": 0.076,
      "grad_norm": 4.308643817901611,
      "learning_rate": 1.9493333333333335e-05,
      "loss": 1.8466,
      "step": 380
    },
    {
      "epoch": 0.078,
      "grad_norm": 3.0250484943389893,
      "learning_rate": 1.948e-05,
      "loss": 2.0512,
      "step": 390
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.127754211425781,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 2.0319,
      "step": 400
    },
    {
      "epoch": 0.082,
      "grad_norm": 5.030002593994141,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 1.8949,
      "step": 410
    },
    {
      "epoch": 0.084,
      "grad_norm": 5.678959846496582,
      "learning_rate": 1.944e-05,
      "loss": 2.2198,
      "step": 420
    },
    {
      "epoch": 0.086,
      "grad_norm": 2.6127769947052,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 1.9985,
      "step": 430
    },
    {
      "epoch": 0.088,
      "grad_norm": 3.278700828552246,
      "learning_rate": 1.9413333333333336e-05,
      "loss": 2.1975,
      "step": 440
    },
    {
      "epoch": 0.09,
      "grad_norm": 3.6387929916381836,
      "learning_rate": 1.94e-05,
      "loss": 1.7765,
      "step": 450
    },
    {
      "epoch": 0.092,
      "grad_norm": 3.4886350631713867,
      "learning_rate": 1.938666666666667e-05,
      "loss": 3.3004,
      "step": 460
    },
    {
      "epoch": 0.094,
      "grad_norm": 4.1661906242370605,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 2.5269,
      "step": 470
    },
    {
      "epoch": 0.096,
      "grad_norm": 3.957200288772583,
      "learning_rate": 1.936e-05,
      "loss": 2.1244,
      "step": 480
    },
    {
      "epoch": 0.098,
      "grad_norm": 3.184413433074951,
      "learning_rate": 1.934666666666667e-05,
      "loss": 2.0662,
      "step": 490
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.5362372398376465,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 2.3038,
      "step": 500
    },
    {
      "epoch": 0.102,
      "grad_norm": 3.2516703605651855,
      "learning_rate": 1.932e-05,
      "loss": 1.7285,
      "step": 510
    },
    {
      "epoch": 0.104,
      "grad_norm": 5.335220813751221,
      "learning_rate": 1.930666666666667e-05,
      "loss": 2.0405,
      "step": 520
    },
    {
      "epoch": 0.106,
      "grad_norm": 4.41134786605835,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 1.7893,
      "step": 530
    },
    {
      "epoch": 0.108,
      "grad_norm": 3.1980345249176025,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 1.8937,
      "step": 540
    },
    {
      "epoch": 0.11,
      "grad_norm": 2.3896374702453613,
      "learning_rate": 1.926666666666667e-05,
      "loss": 2.3924,
      "step": 550
    },
    {
      "epoch": 0.112,
      "grad_norm": 2.8609020709991455,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 1.5915,
      "step": 560
    },
    {
      "epoch": 0.114,
      "grad_norm": 4.762941360473633,
      "learning_rate": 1.9240000000000002e-05,
      "loss": 2.0324,
      "step": 570
    },
    {
      "epoch": 0.116,
      "grad_norm": 2.221574544906616,
      "learning_rate": 1.922666666666667e-05,
      "loss": 2.096,
      "step": 580
    },
    {
      "epoch": 0.118,
      "grad_norm": 5.904628276824951,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 1.8793,
      "step": 590
    },
    {
      "epoch": 0.12,
      "grad_norm": 5.897082805633545,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.8584,
      "step": 600
    },
    {
      "epoch": 0.122,
      "grad_norm": 5.785332679748535,
      "learning_rate": 1.918666666666667e-05,
      "loss": 2.1061,
      "step": 610
    },
    {
      "epoch": 0.124,
      "grad_norm": 3.1629159450531006,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 1.7466,
      "step": 620
    },
    {
      "epoch": 0.126,
      "grad_norm": 2.3704209327697754,
      "learning_rate": 1.916e-05,
      "loss": 1.9663,
      "step": 630
    },
    {
      "epoch": 0.128,
      "grad_norm": 3.31689453125,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 1.961,
      "step": 640
    },
    {
      "epoch": 0.13,
      "grad_norm": 3.610896348953247,
      "learning_rate": 1.9133333333333335e-05,
      "loss": 1.7826,
      "step": 650
    },
    {
      "epoch": 0.132,
      "grad_norm": 13.25782299041748,
      "learning_rate": 1.912e-05,
      "loss": 2.873,
      "step": 660
    },
    {
      "epoch": 0.134,
      "grad_norm": 3.055048704147339,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 2.189,
      "step": 670
    },
    {
      "epoch": 0.136,
      "grad_norm": 2.849112033843994,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 2.3758,
      "step": 680
    },
    {
      "epoch": 0.138,
      "grad_norm": 4.226700305938721,
      "learning_rate": 1.908e-05,
      "loss": 1.9855,
      "step": 690
    },
    {
      "epoch": 0.14,
      "grad_norm": 3.9780638217926025,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 1.74,
      "step": 700
    },
    {
      "epoch": 0.142,
      "grad_norm": 3.004340410232544,
      "learning_rate": 1.9053333333333336e-05,
      "loss": 2.2575,
      "step": 710
    },
    {
      "epoch": 0.144,
      "grad_norm": 3.234750270843506,
      "learning_rate": 1.904e-05,
      "loss": 1.3992,
      "step": 720
    },
    {
      "epoch": 0.146,
      "grad_norm": 3.0884921550750732,
      "learning_rate": 1.902666666666667e-05,
      "loss": 2.0885,
      "step": 730
    },
    {
      "epoch": 0.148,
      "grad_norm": 2.5099332332611084,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 2.3652,
      "step": 740
    },
    {
      "epoch": 0.15,
      "grad_norm": 3.6737585067749023,
      "learning_rate": 1.9e-05,
      "loss": 2.1133,
      "step": 750
    },
    {
      "epoch": 0.152,
      "grad_norm": 2.556027412414551,
      "learning_rate": 1.898666666666667e-05,
      "loss": 1.7807,
      "step": 760
    },
    {
      "epoch": 0.154,
      "grad_norm": 4.109370708465576,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 2.4921,
      "step": 770
    },
    {
      "epoch": 0.156,
      "grad_norm": 4.093138694763184,
      "learning_rate": 1.896e-05,
      "loss": 1.8115,
      "step": 780
    },
    {
      "epoch": 0.158,
      "grad_norm": 4.400282382965088,
      "learning_rate": 1.894666666666667e-05,
      "loss": 2.1127,
      "step": 790
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.188243389129639,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 1.6809,
      "step": 800
    },
    {
      "epoch": 0.162,
      "grad_norm": 3.304621696472168,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 2.136,
      "step": 810
    },
    {
      "epoch": 0.164,
      "grad_norm": 3.330848455429077,
      "learning_rate": 1.890666666666667e-05,
      "loss": 1.9409,
      "step": 820
    },
    {
      "epoch": 0.166,
      "grad_norm": 2.131016254425049,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 2.0695,
      "step": 830
    },
    {
      "epoch": 0.168,
      "grad_norm": 4.3536882400512695,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 1.8022,
      "step": 840
    },
    {
      "epoch": 0.17,
      "grad_norm": 2.6919355392456055,
      "learning_rate": 1.886666666666667e-05,
      "loss": 2.229,
      "step": 850
    },
    {
      "epoch": 0.172,
      "grad_norm": 7.296422004699707,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 2.0455,
      "step": 860
    },
    {
      "epoch": 0.174,
      "grad_norm": 4.929528713226318,
      "learning_rate": 1.884e-05,
      "loss": 2.0014,
      "step": 870
    },
    {
      "epoch": 0.176,
      "grad_norm": 3.425238847732544,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 2.2385,
      "step": 880
    },
    {
      "epoch": 0.178,
      "grad_norm": 3.082510471343994,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 1.7546,
      "step": 890
    },
    {
      "epoch": 0.18,
      "grad_norm": 9.187797546386719,
      "learning_rate": 1.88e-05,
      "loss": 1.9066,
      "step": 900
    },
    {
      "epoch": 0.182,
      "grad_norm": 3.1151609420776367,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 2.368,
      "step": 910
    },
    {
      "epoch": 0.184,
      "grad_norm": 3.3998489379882812,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 1.9576,
      "step": 920
    },
    {
      "epoch": 0.186,
      "grad_norm": 3.110806465148926,
      "learning_rate": 1.876e-05,
      "loss": 1.7478,
      "step": 930
    },
    {
      "epoch": 0.188,
      "grad_norm": 7.659893989562988,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 1.5015,
      "step": 940
    },
    {
      "epoch": 0.19,
      "grad_norm": 3.6938884258270264,
      "learning_rate": 1.8733333333333336e-05,
      "loss": 1.4674,
      "step": 950
    },
    {
      "epoch": 0.192,
      "grad_norm": 3.2413055896759033,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 2.346,
      "step": 960
    },
    {
      "epoch": 0.194,
      "grad_norm": 2.592928647994995,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 1.4379,
      "step": 970
    },
    {
      "epoch": 0.196,
      "grad_norm": 4.662699222564697,
      "learning_rate": 1.8693333333333333e-05,
      "loss": 1.826,
      "step": 980
    },
    {
      "epoch": 0.198,
      "grad_norm": 3.1884820461273193,
      "learning_rate": 1.8680000000000004e-05,
      "loss": 2.2071,
      "step": 990
    },
    {
      "epoch": 0.2,
      "grad_norm": 3.8884642124176025,
      "learning_rate": 1.866666666666667e-05,
      "loss": 2.2223,
      "step": 1000
    },
    {
      "epoch": 0.202,
      "grad_norm": 3.4911067485809326,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 2.217,
      "step": 1010
    },
    {
      "epoch": 0.204,
      "grad_norm": 15.064796447753906,
      "learning_rate": 1.864e-05,
      "loss": 1.8394,
      "step": 1020
    },
    {
      "epoch": 0.206,
      "grad_norm": 3.9083824157714844,
      "learning_rate": 1.862666666666667e-05,
      "loss": 1.7862,
      "step": 1030
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.765244960784912,
      "learning_rate": 1.8613333333333334e-05,
      "loss": 2.089,
      "step": 1040
    },
    {
      "epoch": 0.21,
      "grad_norm": 4.464597225189209,
      "learning_rate": 1.86e-05,
      "loss": 1.578,
      "step": 1050
    },
    {
      "epoch": 0.212,
      "grad_norm": 3.7550764083862305,
      "learning_rate": 1.858666666666667e-05,
      "loss": 2.2663,
      "step": 1060
    },
    {
      "epoch": 0.214,
      "grad_norm": 4.294982433319092,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 1.5691,
      "step": 1070
    },
    {
      "epoch": 0.216,
      "grad_norm": 2.593494415283203,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 2.0601,
      "step": 1080
    },
    {
      "epoch": 0.218,
      "grad_norm": 4.324461936950684,
      "learning_rate": 1.854666666666667e-05,
      "loss": 1.7304,
      "step": 1090
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.7818732261657715,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 1.9013,
      "step": 1100
    },
    {
      "epoch": 0.222,
      "grad_norm": 4.877279281616211,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 2.3862,
      "step": 1110
    },
    {
      "epoch": 0.224,
      "grad_norm": 2.799337387084961,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 1.4874,
      "step": 1120
    },
    {
      "epoch": 0.226,
      "grad_norm": 3.363156795501709,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 1.9673,
      "step": 1130
    },
    {
      "epoch": 0.228,
      "grad_norm": 4.994451522827148,
      "learning_rate": 1.8480000000000003e-05,
      "loss": 1.8732,
      "step": 1140
    },
    {
      "epoch": 0.23,
      "grad_norm": 2.3984670639038086,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 1.6034,
      "step": 1150
    },
    {
      "epoch": 0.232,
      "grad_norm": 3.3874857425689697,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 2.2172,
      "step": 1160
    },
    {
      "epoch": 0.234,
      "grad_norm": 3.2264599800109863,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 1.8387,
      "step": 1170
    },
    {
      "epoch": 0.236,
      "grad_norm": 3.5853192806243896,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 1.9764,
      "step": 1180
    },
    {
      "epoch": 0.238,
      "grad_norm": 3.7175559997558594,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 1.5404,
      "step": 1190
    },
    {
      "epoch": 0.24,
      "grad_norm": 5.031343460083008,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 2.1002,
      "step": 1200
    },
    {
      "epoch": 0.242,
      "grad_norm": 2.5192320346832275,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 1.5651,
      "step": 1210
    },
    {
      "epoch": 0.244,
      "grad_norm": 3.093653440475464,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 1.5976,
      "step": 1220
    },
    {
      "epoch": 0.246,
      "grad_norm": 2.444549322128296,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 1.9347,
      "step": 1230
    },
    {
      "epoch": 0.248,
      "grad_norm": 3.8838088512420654,
      "learning_rate": 1.834666666666667e-05,
      "loss": 2.0002,
      "step": 1240
    },
    {
      "epoch": 0.25,
      "grad_norm": 2.5986366271972656,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 1.5142,
      "step": 1250
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.61578369140625,
      "learning_rate": 1.832e-05,
      "loss": 1.8984,
      "step": 1260
    },
    {
      "epoch": 0.254,
      "grad_norm": 2.459899425506592,
      "learning_rate": 1.830666666666667e-05,
      "loss": 1.9466,
      "step": 1270
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.824237108230591,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 1.9053,
      "step": 1280
    },
    {
      "epoch": 0.258,
      "grad_norm": 2.7822790145874023,
      "learning_rate": 1.828e-05,
      "loss": 1.5296,
      "step": 1290
    },
    {
      "epoch": 0.26,
      "grad_norm": 2.3074960708618164,
      "learning_rate": 1.826666666666667e-05,
      "loss": 1.7969,
      "step": 1300
    },
    {
      "epoch": 0.262,
      "grad_norm": 4.093411922454834,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 2.197,
      "step": 1310
    },
    {
      "epoch": 0.264,
      "grad_norm": 3.4223740100860596,
      "learning_rate": 1.824e-05,
      "loss": 2.0389,
      "step": 1320
    },
    {
      "epoch": 0.266,
      "grad_norm": 4.6114420890808105,
      "learning_rate": 1.822666666666667e-05,
      "loss": 1.8563,
      "step": 1330
    },
    {
      "epoch": 0.268,
      "grad_norm": 2.768156051635742,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 1.5774,
      "step": 1340
    },
    {
      "epoch": 0.27,
      "grad_norm": 4.128940105438232,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 1.6418,
      "step": 1350
    },
    {
      "epoch": 0.272,
      "grad_norm": 2.6647210121154785,
      "learning_rate": 1.8186666666666666e-05,
      "loss": 2.0687,
      "step": 1360
    },
    {
      "epoch": 0.274,
      "grad_norm": 2.542572021484375,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 2.0797,
      "step": 1370
    },
    {
      "epoch": 0.276,
      "grad_norm": 2.920165538787842,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 2.5652,
      "step": 1380
    },
    {
      "epoch": 0.278,
      "grad_norm": 3.1442039012908936,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 1.5834,
      "step": 1390
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.2594590187072754,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 1.4918,
      "step": 1400
    },
    {
      "epoch": 0.282,
      "grad_norm": 4.071898460388184,
      "learning_rate": 1.8120000000000003e-05,
      "loss": 1.7079,
      "step": 1410
    },
    {
      "epoch": 0.284,
      "grad_norm": 3.6377930641174316,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 2.1158,
      "step": 1420
    },
    {
      "epoch": 0.286,
      "grad_norm": 3.536879062652588,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 1.7539,
      "step": 1430
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.5160062313079834,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 1.7193,
      "step": 1440
    },
    {
      "epoch": 0.29,
      "grad_norm": 4.614177227020264,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 2.5539,
      "step": 1450
    },
    {
      "epoch": 0.292,
      "grad_norm": 2.852001190185547,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 1.6431,
      "step": 1460
    },
    {
      "epoch": 0.294,
      "grad_norm": 4.338400840759277,
      "learning_rate": 1.8040000000000003e-05,
      "loss": 1.998,
      "step": 1470
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.1278562545776367,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 1.8292,
      "step": 1480
    },
    {
      "epoch": 0.298,
      "grad_norm": 2.4486446380615234,
      "learning_rate": 1.8013333333333333e-05,
      "loss": 1.5986,
      "step": 1490
    },
    {
      "epoch": 0.3,
      "grad_norm": 3.056105613708496,
      "learning_rate": 1.8e-05,
      "loss": 1.5386,
      "step": 1500
    },
    {
      "epoch": 0.302,
      "grad_norm": 3.210745334625244,
      "learning_rate": 1.798666666666667e-05,
      "loss": 2.2254,
      "step": 1510
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.3349926471710205,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 1.3939,
      "step": 1520
    },
    {
      "epoch": 0.306,
      "grad_norm": 1.5820558071136475,
      "learning_rate": 1.796e-05,
      "loss": 1.9111,
      "step": 1530
    },
    {
      "epoch": 0.308,
      "grad_norm": 2.6897013187408447,
      "learning_rate": 1.794666666666667e-05,
      "loss": 1.6354,
      "step": 1540
    },
    {
      "epoch": 0.31,
      "grad_norm": 3.058403491973877,
      "learning_rate": 1.7933333333333333e-05,
      "loss": 1.7189,
      "step": 1550
    },
    {
      "epoch": 0.312,
      "grad_norm": 8.109725952148438,
      "learning_rate": 1.792e-05,
      "loss": 1.9589,
      "step": 1560
    },
    {
      "epoch": 0.314,
      "grad_norm": 2.41961669921875,
      "learning_rate": 1.790666666666667e-05,
      "loss": 2.0893,
      "step": 1570
    },
    {
      "epoch": 0.316,
      "grad_norm": 3.9316155910491943,
      "learning_rate": 1.7893333333333337e-05,
      "loss": 2.3607,
      "step": 1580
    },
    {
      "epoch": 0.318,
      "grad_norm": 4.061583042144775,
      "learning_rate": 1.788e-05,
      "loss": 1.5277,
      "step": 1590
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.884312152862549,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 1.4649,
      "step": 1600
    },
    {
      "epoch": 0.322,
      "grad_norm": 3.4531502723693848,
      "learning_rate": 1.7853333333333337e-05,
      "loss": 2.4229,
      "step": 1610
    },
    {
      "epoch": 0.324,
      "grad_norm": 3.7051010131835938,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 1.858,
      "step": 1620
    },
    {
      "epoch": 0.326,
      "grad_norm": 2.7906692028045654,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 1.8628,
      "step": 1630
    },
    {
      "epoch": 0.328,
      "grad_norm": 2.7064199447631836,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 1.7033,
      "step": 1640
    },
    {
      "epoch": 0.33,
      "grad_norm": 3.6684818267822266,
      "learning_rate": 1.7800000000000002e-05,
      "loss": 2.028,
      "step": 1650
    },
    {
      "epoch": 0.332,
      "grad_norm": 1.804964303970337,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 2.0755,
      "step": 1660
    },
    {
      "epoch": 0.334,
      "grad_norm": 3.3178837299346924,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 1.6901,
      "step": 1670
    },
    {
      "epoch": 0.336,
      "grad_norm": 3.2161409854888916,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 2.025,
      "step": 1680
    },
    {
      "epoch": 0.338,
      "grad_norm": 2.0100884437561035,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 1.4381,
      "step": 1690
    },
    {
      "epoch": 0.34,
      "grad_norm": 4.4496541023254395,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 1.9579,
      "step": 1700
    },
    {
      "epoch": 0.342,
      "grad_norm": 2.3774876594543457,
      "learning_rate": 1.7720000000000003e-05,
      "loss": 1.6223,
      "step": 1710
    },
    {
      "epoch": 0.344,
      "grad_norm": 2.914181709289551,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 1.5151,
      "step": 1720
    },
    {
      "epoch": 0.346,
      "grad_norm": 3.4519667625427246,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 1.7169,
      "step": 1730
    },
    {
      "epoch": 0.348,
      "grad_norm": 2.547288417816162,
      "learning_rate": 1.768e-05,
      "loss": 1.9627,
      "step": 1740
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.643556833267212,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 1.7939,
      "step": 1750
    },
    {
      "epoch": 0.352,
      "grad_norm": 3.941659688949585,
      "learning_rate": 1.7653333333333336e-05,
      "loss": 2.2502,
      "step": 1760
    },
    {
      "epoch": 0.354,
      "grad_norm": 4.0174241065979,
      "learning_rate": 1.764e-05,
      "loss": 1.9713,
      "step": 1770
    },
    {
      "epoch": 0.356,
      "grad_norm": 3.478303909301758,
      "learning_rate": 1.762666666666667e-05,
      "loss": 1.8057,
      "step": 1780
    },
    {
      "epoch": 0.358,
      "grad_norm": 4.534031867980957,
      "learning_rate": 1.7613333333333336e-05,
      "loss": 1.7069,
      "step": 1790
    },
    {
      "epoch": 0.36,
      "grad_norm": 3.5429821014404297,
      "learning_rate": 1.76e-05,
      "loss": 1.3955,
      "step": 1800
    },
    {
      "epoch": 0.362,
      "grad_norm": 3.558926820755005,
      "learning_rate": 1.758666666666667e-05,
      "loss": 1.5635,
      "step": 1810
    },
    {
      "epoch": 0.364,
      "grad_norm": 4.829414367675781,
      "learning_rate": 1.7573333333333337e-05,
      "loss": 2.3228,
      "step": 1820
    },
    {
      "epoch": 0.366,
      "grad_norm": 3.8511555194854736,
      "learning_rate": 1.756e-05,
      "loss": 2.0434,
      "step": 1830
    },
    {
      "epoch": 0.368,
      "grad_norm": 4.258585453033447,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 1.8127,
      "step": 1840
    },
    {
      "epoch": 0.37,
      "grad_norm": 3.0493323802948,
      "learning_rate": 1.7533333333333337e-05,
      "loss": 1.6374,
      "step": 1850
    },
    {
      "epoch": 0.372,
      "grad_norm": 4.267048358917236,
      "learning_rate": 1.752e-05,
      "loss": 1.6766,
      "step": 1860
    },
    {
      "epoch": 0.374,
      "grad_norm": 2.895825147628784,
      "learning_rate": 1.7506666666666666e-05,
      "loss": 1.7348,
      "step": 1870
    },
    {
      "epoch": 0.376,
      "grad_norm": 4.219796657562256,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 1.6588,
      "step": 1880
    },
    {
      "epoch": 0.378,
      "grad_norm": 4.56646728515625,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 1.8114,
      "step": 1890
    },
    {
      "epoch": 0.38,
      "grad_norm": 2.9212605953216553,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 1.7458,
      "step": 1900
    },
    {
      "epoch": 0.382,
      "grad_norm": 2.930478096008301,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 2.295,
      "step": 1910
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.9285669326782227,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 1.8683,
      "step": 1920
    },
    {
      "epoch": 0.386,
      "grad_norm": 3.500159740447998,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 1.9466,
      "step": 1930
    },
    {
      "epoch": 0.388,
      "grad_norm": 3.3065640926361084,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 1.8865,
      "step": 1940
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.378676414489746,
      "learning_rate": 1.7400000000000003e-05,
      "loss": 1.5959,
      "step": 1950
    },
    {
      "epoch": 0.392,
      "grad_norm": 2.227027654647827,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 2.032,
      "step": 1960
    },
    {
      "epoch": 0.394,
      "grad_norm": 2.4756460189819336,
      "learning_rate": 1.7373333333333335e-05,
      "loss": 1.5156,
      "step": 1970
    },
    {
      "epoch": 0.396,
      "grad_norm": 2.581610918045044,
      "learning_rate": 1.736e-05,
      "loss": 1.3306,
      "step": 1980
    },
    {
      "epoch": 0.398,
      "grad_norm": 2.182302951812744,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 1.5943,
      "step": 1990
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.614492177963257,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.7864,
      "step": 2000
    },
    {
      "epoch": 0.402,
      "grad_norm": 2.359668016433716,
      "learning_rate": 1.732e-05,
      "loss": 2.0728,
      "step": 2010
    },
    {
      "epoch": 0.404,
      "grad_norm": 2.2759506702423096,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 1.717,
      "step": 2020
    },
    {
      "epoch": 0.406,
      "grad_norm": 3.4949800968170166,
      "learning_rate": 1.7293333333333336e-05,
      "loss": 1.9012,
      "step": 2030
    },
    {
      "epoch": 0.408,
      "grad_norm": 3.8563504219055176,
      "learning_rate": 1.728e-05,
      "loss": 1.8054,
      "step": 2040
    },
    {
      "epoch": 0.41,
      "grad_norm": 3.0786333084106445,
      "learning_rate": 1.726666666666667e-05,
      "loss": 1.6986,
      "step": 2050
    },
    {
      "epoch": 0.412,
      "grad_norm": 2.6686909198760986,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 1.9789,
      "step": 2060
    },
    {
      "epoch": 0.414,
      "grad_norm": 2.266721725463867,
      "learning_rate": 1.724e-05,
      "loss": 1.2902,
      "step": 2070
    },
    {
      "epoch": 0.416,
      "grad_norm": 4.5177001953125,
      "learning_rate": 1.7226666666666665e-05,
      "loss": 2.0099,
      "step": 2080
    },
    {
      "epoch": 0.418,
      "grad_norm": 3.3463168144226074,
      "learning_rate": 1.7213333333333337e-05,
      "loss": 1.6588,
      "step": 2090
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.621264696121216,
      "learning_rate": 1.72e-05,
      "loss": 1.7181,
      "step": 2100
    },
    {
      "epoch": 0.422,
      "grad_norm": 2.460571527481079,
      "learning_rate": 1.7186666666666666e-05,
      "loss": 1.4824,
      "step": 2110
    },
    {
      "epoch": 0.424,
      "grad_norm": 3.314383029937744,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 2.0034,
      "step": 2120
    },
    {
      "epoch": 0.426,
      "grad_norm": 3.8082077503204346,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 1.6174,
      "step": 2130
    },
    {
      "epoch": 0.428,
      "grad_norm": 2.396623373031616,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 1.656,
      "step": 2140
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.904344081878662,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 1.6044,
      "step": 2150
    },
    {
      "epoch": 0.432,
      "grad_norm": 3.8851022720336914,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 2.4237,
      "step": 2160
    },
    {
      "epoch": 0.434,
      "grad_norm": 3.3487703800201416,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 1.8224,
      "step": 2170
    },
    {
      "epoch": 0.436,
      "grad_norm": 2.5160367488861084,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 1.9988,
      "step": 2180
    },
    {
      "epoch": 0.438,
      "grad_norm": 2.7597954273223877,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 2.3755,
      "step": 2190
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.9060161113739014,
      "learning_rate": 1.706666666666667e-05,
      "loss": 1.5803,
      "step": 2200
    },
    {
      "epoch": 0.442,
      "grad_norm": 3.5170562267303467,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 1.8968,
      "step": 2210
    },
    {
      "epoch": 0.444,
      "grad_norm": 2.7092416286468506,
      "learning_rate": 1.704e-05,
      "loss": 1.5862,
      "step": 2220
    },
    {
      "epoch": 0.446,
      "grad_norm": 2.5545108318328857,
      "learning_rate": 1.702666666666667e-05,
      "loss": 1.6362,
      "step": 2230
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.926206350326538,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 2.1712,
      "step": 2240
    },
    {
      "epoch": 0.45,
      "grad_norm": 2.3281352519989014,
      "learning_rate": 1.7e-05,
      "loss": 1.8032,
      "step": 2250
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.5739264488220215,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 1.7686,
      "step": 2260
    },
    {
      "epoch": 0.454,
      "grad_norm": 3.2008142471313477,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 1.8168,
      "step": 2270
    },
    {
      "epoch": 0.456,
      "grad_norm": 4.313835144042969,
      "learning_rate": 1.696e-05,
      "loss": 1.7692,
      "step": 2280
    },
    {
      "epoch": 0.458,
      "grad_norm": 2.034925937652588,
      "learning_rate": 1.6946666666666668e-05,
      "loss": 2.0287,
      "step": 2290
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.5842654705047607,
      "learning_rate": 1.6933333333333336e-05,
      "loss": 1.9909,
      "step": 2300
    },
    {
      "epoch": 0.462,
      "grad_norm": 3.6704065799713135,
      "learning_rate": 1.692e-05,
      "loss": 1.8708,
      "step": 2310
    },
    {
      "epoch": 0.464,
      "grad_norm": 6.3101091384887695,
      "learning_rate": 1.690666666666667e-05,
      "loss": 1.7532,
      "step": 2320
    },
    {
      "epoch": 0.466,
      "grad_norm": 2.686486005783081,
      "learning_rate": 1.6893333333333336e-05,
      "loss": 1.4805,
      "step": 2330
    },
    {
      "epoch": 0.468,
      "grad_norm": 4.028355121612549,
      "learning_rate": 1.688e-05,
      "loss": 1.6876,
      "step": 2340
    },
    {
      "epoch": 0.47,
      "grad_norm": 2.845961093902588,
      "learning_rate": 1.686666666666667e-05,
      "loss": 1.8153,
      "step": 2350
    },
    {
      "epoch": 0.472,
      "grad_norm": 4.34791898727417,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 1.5455,
      "step": 2360
    },
    {
      "epoch": 0.474,
      "grad_norm": 3.311006546020508,
      "learning_rate": 1.684e-05,
      "loss": 1.5054,
      "step": 2370
    },
    {
      "epoch": 0.476,
      "grad_norm": 2.9857559204101562,
      "learning_rate": 1.682666666666667e-05,
      "loss": 2.2386,
      "step": 2380
    },
    {
      "epoch": 0.478,
      "grad_norm": 2.9691741466522217,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 1.9846,
      "step": 2390
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.1856980323791504,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.5297,
      "step": 2400
    },
    {
      "epoch": 0.482,
      "grad_norm": 2.8985557556152344,
      "learning_rate": 1.678666666666667e-05,
      "loss": 1.7264,
      "step": 2410
    },
    {
      "epoch": 0.484,
      "grad_norm": 2.304614782333374,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 1.8341,
      "step": 2420
    },
    {
      "epoch": 0.486,
      "grad_norm": 3.495650291442871,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 1.9062,
      "step": 2430
    },
    {
      "epoch": 0.488,
      "grad_norm": 3.8042635917663574,
      "learning_rate": 1.674666666666667e-05,
      "loss": 1.6795,
      "step": 2440
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.9729301929473877,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 1.5426,
      "step": 2450
    },
    {
      "epoch": 0.492,
      "grad_norm": 2.4024198055267334,
      "learning_rate": 1.672e-05,
      "loss": 1.8503,
      "step": 2460
    },
    {
      "epoch": 0.494,
      "grad_norm": 3.9699249267578125,
      "learning_rate": 1.670666666666667e-05,
      "loss": 1.5776,
      "step": 2470
    },
    {
      "epoch": 0.496,
      "grad_norm": 1.778570532798767,
      "learning_rate": 1.6693333333333335e-05,
      "loss": 1.2787,
      "step": 2480
    },
    {
      "epoch": 0.498,
      "grad_norm": 2.789041757583618,
      "learning_rate": 1.668e-05,
      "loss": 1.612,
      "step": 2490
    },
    {
      "epoch": 0.5,
      "grad_norm": 2.254333734512329,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.5114,
      "step": 2500
    },
    {
      "epoch": 0.502,
      "grad_norm": 2.986600160598755,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 1.6511,
      "step": 2510
    },
    {
      "epoch": 0.504,
      "grad_norm": 2.341705322265625,
      "learning_rate": 1.664e-05,
      "loss": 1.5404,
      "step": 2520
    },
    {
      "epoch": 0.506,
      "grad_norm": 3.0809035301208496,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 1.8436,
      "step": 2530
    },
    {
      "epoch": 0.508,
      "grad_norm": 2.7885053157806396,
      "learning_rate": 1.6613333333333336e-05,
      "loss": 1.5407,
      "step": 2540
    },
    {
      "epoch": 0.51,
      "grad_norm": 2.540008544921875,
      "learning_rate": 1.66e-05,
      "loss": 1.9012,
      "step": 2550
    },
    {
      "epoch": 0.512,
      "grad_norm": 3.941565752029419,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 2.2328,
      "step": 2560
    },
    {
      "epoch": 0.514,
      "grad_norm": 4.1015448570251465,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 1.8172,
      "step": 2570
    },
    {
      "epoch": 0.516,
      "grad_norm": 2.9736328125,
      "learning_rate": 1.656e-05,
      "loss": 2.2331,
      "step": 2580
    },
    {
      "epoch": 0.518,
      "grad_norm": 2.9972872734069824,
      "learning_rate": 1.654666666666667e-05,
      "loss": 1.933,
      "step": 2590
    },
    {
      "epoch": 0.52,
      "grad_norm": 2.83917236328125,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 2.3912,
      "step": 2600
    },
    {
      "epoch": 0.522,
      "grad_norm": 2.7489616870880127,
      "learning_rate": 1.652e-05,
      "loss": 1.9455,
      "step": 2610
    },
    {
      "epoch": 0.524,
      "grad_norm": 2.8985636234283447,
      "learning_rate": 1.650666666666667e-05,
      "loss": 1.2581,
      "step": 2620
    },
    {
      "epoch": 0.526,
      "grad_norm": 3.845078468322754,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 1.3293,
      "step": 2630
    },
    {
      "epoch": 0.528,
      "grad_norm": 3.1534852981567383,
      "learning_rate": 1.648e-05,
      "loss": 1.8095,
      "step": 2640
    },
    {
      "epoch": 0.53,
      "grad_norm": 2.6875295639038086,
      "learning_rate": 1.646666666666667e-05,
      "loss": 1.6512,
      "step": 2650
    },
    {
      "epoch": 0.532,
      "grad_norm": 3.009636878967285,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 1.4618,
      "step": 2660
    },
    {
      "epoch": 0.534,
      "grad_norm": 3.6574900150299072,
      "learning_rate": 1.6440000000000002e-05,
      "loss": 1.9136,
      "step": 2670
    },
    {
      "epoch": 0.536,
      "grad_norm": 2.965648889541626,
      "learning_rate": 1.642666666666667e-05,
      "loss": 1.6774,
      "step": 2680
    },
    {
      "epoch": 0.538,
      "grad_norm": 2.4630348682403564,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 1.9642,
      "step": 2690
    },
    {
      "epoch": 0.54,
      "grad_norm": 2.897554636001587,
      "learning_rate": 1.64e-05,
      "loss": 1.8217,
      "step": 2700
    },
    {
      "epoch": 0.542,
      "grad_norm": 3.6378304958343506,
      "learning_rate": 1.638666666666667e-05,
      "loss": 2.2205,
      "step": 2710
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.65971040725708,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 1.606,
      "step": 2720
    },
    {
      "epoch": 0.546,
      "grad_norm": 2.8678507804870605,
      "learning_rate": 1.636e-05,
      "loss": 1.6847,
      "step": 2730
    },
    {
      "epoch": 0.548,
      "grad_norm": 2.0746941566467285,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 2.0326,
      "step": 2740
    },
    {
      "epoch": 0.55,
      "grad_norm": 3.467916965484619,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 1.7,
      "step": 2750
    },
    {
      "epoch": 0.552,
      "grad_norm": 4.813721179962158,
      "learning_rate": 1.632e-05,
      "loss": 1.864,
      "step": 2760
    },
    {
      "epoch": 0.554,
      "grad_norm": 2.265315294265747,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 1.9193,
      "step": 2770
    },
    {
      "epoch": 0.556,
      "grad_norm": 2.4052345752716064,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 1.1597,
      "step": 2780
    },
    {
      "epoch": 0.558,
      "grad_norm": 4.350791931152344,
      "learning_rate": 1.628e-05,
      "loss": 2.3893,
      "step": 2790
    },
    {
      "epoch": 0.56,
      "grad_norm": 8.613750457763672,
      "learning_rate": 1.6266666666666668e-05,
      "loss": 1.7302,
      "step": 2800
    },
    {
      "epoch": 0.562,
      "grad_norm": 3.2735683917999268,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 1.4285,
      "step": 2810
    },
    {
      "epoch": 0.564,
      "grad_norm": 3.824629545211792,
      "learning_rate": 1.6240000000000004e-05,
      "loss": 1.6819,
      "step": 2820
    },
    {
      "epoch": 0.566,
      "grad_norm": 1.7658437490463257,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 1.7462,
      "step": 2830
    },
    {
      "epoch": 0.568,
      "grad_norm": 2.832084894180298,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 2.0833,
      "step": 2840
    },
    {
      "epoch": 0.57,
      "grad_norm": 3.0189404487609863,
      "learning_rate": 1.62e-05,
      "loss": 1.925,
      "step": 2850
    },
    {
      "epoch": 0.572,
      "grad_norm": 2.744826555252075,
      "learning_rate": 1.618666666666667e-05,
      "loss": 1.65,
      "step": 2860
    },
    {
      "epoch": 0.574,
      "grad_norm": 4.369922637939453,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 1.6042,
      "step": 2870
    },
    {
      "epoch": 0.576,
      "grad_norm": 2.6018784046173096,
      "learning_rate": 1.616e-05,
      "loss": 1.7802,
      "step": 2880
    },
    {
      "epoch": 0.578,
      "grad_norm": 3.279252290725708,
      "learning_rate": 1.614666666666667e-05,
      "loss": 2.1906,
      "step": 2890
    },
    {
      "epoch": 0.58,
      "grad_norm": 3.279412269592285,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 1.6649,
      "step": 2900
    },
    {
      "epoch": 0.582,
      "grad_norm": 1.975110411643982,
      "learning_rate": 1.612e-05,
      "loss": 1.5778,
      "step": 2910
    },
    {
      "epoch": 0.584,
      "grad_norm": 2.038270950317383,
      "learning_rate": 1.610666666666667e-05,
      "loss": 1.5769,
      "step": 2920
    },
    {
      "epoch": 0.586,
      "grad_norm": 2.1602158546447754,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 1.76,
      "step": 2930
    },
    {
      "epoch": 0.588,
      "grad_norm": 3.2410523891448975,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 1.9175,
      "step": 2940
    },
    {
      "epoch": 0.59,
      "grad_norm": 3.5678882598876953,
      "learning_rate": 1.606666666666667e-05,
      "loss": 1.8302,
      "step": 2950
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.8524012565612793,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 1.7875,
      "step": 2960
    },
    {
      "epoch": 0.594,
      "grad_norm": 3.8375508785247803,
      "learning_rate": 1.6040000000000002e-05,
      "loss": 1.6081,
      "step": 2970
    },
    {
      "epoch": 0.596,
      "grad_norm": 3.2147886753082275,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 1.7238,
      "step": 2980
    },
    {
      "epoch": 0.598,
      "grad_norm": 4.798262596130371,
      "learning_rate": 1.6013333333333335e-05,
      "loss": 2.2339,
      "step": 2990
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.8057045936584473,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9962,
      "step": 3000
    },
    {
      "epoch": 0.602,
      "grad_norm": 2.449049711227417,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 1.2934,
      "step": 3010
    },
    {
      "epoch": 0.604,
      "grad_norm": 2.493577003479004,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 1.3617,
      "step": 3020
    },
    {
      "epoch": 0.606,
      "grad_norm": 2.378695249557495,
      "learning_rate": 1.5960000000000003e-05,
      "loss": 1.745,
      "step": 3030
    },
    {
      "epoch": 0.608,
      "grad_norm": 2.4750137329101562,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 2.0626,
      "step": 3040
    },
    {
      "epoch": 0.61,
      "grad_norm": 2.5298852920532227,
      "learning_rate": 1.5933333333333336e-05,
      "loss": 2.1373,
      "step": 3050
    },
    {
      "epoch": 0.612,
      "grad_norm": 2.447800397872925,
      "learning_rate": 1.5920000000000003e-05,
      "loss": 1.9424,
      "step": 3060
    },
    {
      "epoch": 0.614,
      "grad_norm": 3.1680502891540527,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 1.1087,
      "step": 3070
    },
    {
      "epoch": 0.616,
      "grad_norm": 2.9998180866241455,
      "learning_rate": 1.5893333333333333e-05,
      "loss": 1.9557,
      "step": 3080
    },
    {
      "epoch": 0.618,
      "grad_norm": 3.4062788486480713,
      "learning_rate": 1.588e-05,
      "loss": 2.0713,
      "step": 3090
    },
    {
      "epoch": 0.62,
      "grad_norm": 3.886321544647217,
      "learning_rate": 1.586666666666667e-05,
      "loss": 2.0647,
      "step": 3100
    },
    {
      "epoch": 0.622,
      "grad_norm": 2.47802996635437,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 1.8074,
      "step": 3110
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.479339361190796,
      "learning_rate": 1.584e-05,
      "loss": 1.5201,
      "step": 3120
    },
    {
      "epoch": 0.626,
      "grad_norm": 8.266655921936035,
      "learning_rate": 1.582666666666667e-05,
      "loss": 1.7036,
      "step": 3130
    },
    {
      "epoch": 0.628,
      "grad_norm": 2.123462200164795,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 1.9508,
      "step": 3140
    },
    {
      "epoch": 0.63,
      "grad_norm": 2.6636641025543213,
      "learning_rate": 1.58e-05,
      "loss": 2.6399,
      "step": 3150
    },
    {
      "epoch": 0.632,
      "grad_norm": 4.3122687339782715,
      "learning_rate": 1.578666666666667e-05,
      "loss": 1.6094,
      "step": 3160
    },
    {
      "epoch": 0.634,
      "grad_norm": 3.4777255058288574,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 1.5903,
      "step": 3170
    },
    {
      "epoch": 0.636,
      "grad_norm": 3.1960418224334717,
      "learning_rate": 1.576e-05,
      "loss": 1.5219,
      "step": 3180
    },
    {
      "epoch": 0.638,
      "grad_norm": 3.3222196102142334,
      "learning_rate": 1.574666666666667e-05,
      "loss": 1.5376,
      "step": 3190
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.206387758255005,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 2.1314,
      "step": 3200
    },
    {
      "epoch": 0.642,
      "grad_norm": 4.626523494720459,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 1.7849,
      "step": 3210
    },
    {
      "epoch": 0.644,
      "grad_norm": 3.990046501159668,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 2.0939,
      "step": 3220
    },
    {
      "epoch": 0.646,
      "grad_norm": 2.534451961517334,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 1.2243,
      "step": 3230
    },
    {
      "epoch": 0.648,
      "grad_norm": 6.672447204589844,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 1.7067,
      "step": 3240
    },
    {
      "epoch": 0.65,
      "grad_norm": 3.453756093978882,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 2.1963,
      "step": 3250
    },
    {
      "epoch": 0.652,
      "grad_norm": 4.587095737457275,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 1.3727,
      "step": 3260
    },
    {
      "epoch": 0.654,
      "grad_norm": 3.5713698863983154,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 1.7763,
      "step": 3270
    },
    {
      "epoch": 0.656,
      "grad_norm": 3.465970039367676,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 1.7247,
      "step": 3280
    },
    {
      "epoch": 0.658,
      "grad_norm": 4.76546049118042,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 1.8892,
      "step": 3290
    },
    {
      "epoch": 0.66,
      "grad_norm": 6.290944576263428,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 1.311,
      "step": 3300
    },
    {
      "epoch": 0.662,
      "grad_norm": 2.6324315071105957,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 1.8258,
      "step": 3310
    },
    {
      "epoch": 0.664,
      "grad_norm": 3.224174976348877,
      "learning_rate": 1.5573333333333332e-05,
      "loss": 1.5962,
      "step": 3320
    },
    {
      "epoch": 0.666,
      "grad_norm": 2.3590023517608643,
      "learning_rate": 1.556e-05,
      "loss": 1.6731,
      "step": 3330
    },
    {
      "epoch": 0.668,
      "grad_norm": 3.3770456314086914,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 1.4773,
      "step": 3340
    },
    {
      "epoch": 0.67,
      "grad_norm": 2.172579765319824,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 1.3501,
      "step": 3350
    },
    {
      "epoch": 0.672,
      "grad_norm": 3.4265503883361816,
      "learning_rate": 1.552e-05,
      "loss": 1.854,
      "step": 3360
    },
    {
      "epoch": 0.674,
      "grad_norm": 3.2938666343688965,
      "learning_rate": 1.550666666666667e-05,
      "loss": 1.7483,
      "step": 3370
    },
    {
      "epoch": 0.676,
      "grad_norm": 3.1434998512268066,
      "learning_rate": 1.5493333333333333e-05,
      "loss": 1.6161,
      "step": 3380
    },
    {
      "epoch": 0.678,
      "grad_norm": 4.143539905548096,
      "learning_rate": 1.548e-05,
      "loss": 1.7231,
      "step": 3390
    },
    {
      "epoch": 0.68,
      "grad_norm": 3.409224033355713,
      "learning_rate": 1.546666666666667e-05,
      "loss": 1.4265,
      "step": 3400
    },
    {
      "epoch": 0.682,
      "grad_norm": 3.223731756210327,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 1.7784,
      "step": 3410
    },
    {
      "epoch": 0.684,
      "grad_norm": 5.024901390075684,
      "learning_rate": 1.544e-05,
      "loss": 2.1411,
      "step": 3420
    },
    {
      "epoch": 0.686,
      "grad_norm": 4.111581325531006,
      "learning_rate": 1.542666666666667e-05,
      "loss": 1.279,
      "step": 3430
    },
    {
      "epoch": 0.688,
      "grad_norm": 1.928097128868103,
      "learning_rate": 1.5413333333333337e-05,
      "loss": 1.67,
      "step": 3440
    },
    {
      "epoch": 0.69,
      "grad_norm": 4.4124956130981445,
      "learning_rate": 1.54e-05,
      "loss": 1.6308,
      "step": 3450
    },
    {
      "epoch": 0.692,
      "grad_norm": 4.069614887237549,
      "learning_rate": 1.5386666666666666e-05,
      "loss": 1.9774,
      "step": 3460
    },
    {
      "epoch": 0.694,
      "grad_norm": 3.2951231002807617,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 1.8096,
      "step": 3470
    },
    {
      "epoch": 0.696,
      "grad_norm": 2.1809804439544678,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 1.5314,
      "step": 3480
    },
    {
      "epoch": 0.698,
      "grad_norm": 3.347381114959717,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 1.3534,
      "step": 3490
    },
    {
      "epoch": 0.7,
      "grad_norm": 3.4764676094055176,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 1.5553,
      "step": 3500
    },
    {
      "epoch": 0.702,
      "grad_norm": 3.19067645072937,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 1.7381,
      "step": 3510
    },
    {
      "epoch": 0.704,
      "grad_norm": 3.008580446243286,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 1.9152,
      "step": 3520
    },
    {
      "epoch": 0.706,
      "grad_norm": 2.919811725616455,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 1.6279,
      "step": 3530
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.9334304332733154,
      "learning_rate": 1.5280000000000003e-05,
      "loss": 1.6373,
      "step": 3540
    },
    {
      "epoch": 0.71,
      "grad_norm": 1.9991401433944702,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 1.3667,
      "step": 3550
    },
    {
      "epoch": 0.712,
      "grad_norm": 3.36309552192688,
      "learning_rate": 1.5253333333333335e-05,
      "loss": 1.7617,
      "step": 3560
    },
    {
      "epoch": 0.714,
      "grad_norm": 2.952059030532837,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 2.0249,
      "step": 3570
    },
    {
      "epoch": 0.716,
      "grad_norm": 4.543336868286133,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 2.248,
      "step": 3580
    },
    {
      "epoch": 0.718,
      "grad_norm": 2.8808822631835938,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 1.5903,
      "step": 3590
    },
    {
      "epoch": 0.72,
      "grad_norm": 5.64137601852417,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 1.5079,
      "step": 3600
    },
    {
      "epoch": 0.722,
      "grad_norm": 3.3074142932891846,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 1.693,
      "step": 3610
    },
    {
      "epoch": 0.724,
      "grad_norm": 3.8758203983306885,
      "learning_rate": 1.5173333333333336e-05,
      "loss": 2.7999,
      "step": 3620
    },
    {
      "epoch": 0.726,
      "grad_norm": 4.204850673675537,
      "learning_rate": 1.516e-05,
      "loss": 1.7462,
      "step": 3630
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.1684060096740723,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 1.7008,
      "step": 3640
    },
    {
      "epoch": 0.73,
      "grad_norm": 2.6650354862213135,
      "learning_rate": 1.5133333333333335e-05,
      "loss": 1.724,
      "step": 3650
    },
    {
      "epoch": 0.732,
      "grad_norm": 2.0776302814483643,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 1.6966,
      "step": 3660
    },
    {
      "epoch": 0.734,
      "grad_norm": 4.16254997253418,
      "learning_rate": 1.5106666666666667e-05,
      "loss": 1.4112,
      "step": 3670
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.529287815093994,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 1.7695,
      "step": 3680
    },
    {
      "epoch": 0.738,
      "grad_norm": 3.275606155395508,
      "learning_rate": 1.5080000000000001e-05,
      "loss": 1.4873,
      "step": 3690
    },
    {
      "epoch": 0.74,
      "grad_norm": 2.5272929668426514,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 1.6889,
      "step": 3700
    },
    {
      "epoch": 0.742,
      "grad_norm": 2.811424732208252,
      "learning_rate": 1.5053333333333335e-05,
      "loss": 2.074,
      "step": 3710
    },
    {
      "epoch": 0.744,
      "grad_norm": 2.88592267036438,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 1.7887,
      "step": 3720
    },
    {
      "epoch": 0.746,
      "grad_norm": 2.560720920562744,
      "learning_rate": 1.5026666666666668e-05,
      "loss": 1.9809,
      "step": 3730
    },
    {
      "epoch": 0.748,
      "grad_norm": 3.029975414276123,
      "learning_rate": 1.5013333333333336e-05,
      "loss": 1.1105,
      "step": 3740
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.8680691719055176,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 1.9753,
      "step": 3750
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.752504348754883,
      "learning_rate": 1.4986666666666667e-05,
      "loss": 1.7606,
      "step": 3760
    },
    {
      "epoch": 0.754,
      "grad_norm": 3.1737170219421387,
      "learning_rate": 1.4973333333333335e-05,
      "loss": 0.9295,
      "step": 3770
    },
    {
      "epoch": 0.756,
      "grad_norm": 2.611449718475342,
      "learning_rate": 1.496e-05,
      "loss": 1.8011,
      "step": 3780
    },
    {
      "epoch": 0.758,
      "grad_norm": 4.438803195953369,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 1.8118,
      "step": 3790
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.969008207321167,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 1.5675,
      "step": 3800
    },
    {
      "epoch": 0.762,
      "grad_norm": 2.939964771270752,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 2.0782,
      "step": 3810
    },
    {
      "epoch": 0.764,
      "grad_norm": 5.317477703094482,
      "learning_rate": 1.4906666666666667e-05,
      "loss": 1.7978,
      "step": 3820
    },
    {
      "epoch": 0.766,
      "grad_norm": 2.5360941886901855,
      "learning_rate": 1.4893333333333335e-05,
      "loss": 1.9425,
      "step": 3830
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.780714511871338,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 1.7683,
      "step": 3840
    },
    {
      "epoch": 0.77,
      "grad_norm": 4.20684814453125,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 2.0087,
      "step": 3850
    },
    {
      "epoch": 0.772,
      "grad_norm": 4.130749225616455,
      "learning_rate": 1.4853333333333336e-05,
      "loss": 1.4044,
      "step": 3860
    },
    {
      "epoch": 0.774,
      "grad_norm": 3.1114377975463867,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 2.1822,
      "step": 3870
    },
    {
      "epoch": 0.776,
      "grad_norm": 2.574552297592163,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 1.8674,
      "step": 3880
    },
    {
      "epoch": 0.778,
      "grad_norm": 3.3884670734405518,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 1.6667,
      "step": 3890
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.8609130382537842,
      "learning_rate": 1.48e-05,
      "loss": 1.3853,
      "step": 3900
    },
    {
      "epoch": 0.782,
      "grad_norm": 2.4751341342926025,
      "learning_rate": 1.4786666666666667e-05,
      "loss": 1.5677,
      "step": 3910
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.0155375003814697,
      "learning_rate": 1.4773333333333335e-05,
      "loss": 1.6141,
      "step": 3920
    },
    {
      "epoch": 0.786,
      "grad_norm": 2.691741943359375,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 1.6976,
      "step": 3930
    },
    {
      "epoch": 0.788,
      "grad_norm": 2.7162282466888428,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 1.3666,
      "step": 3940
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.9255425930023193,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 1.0627,
      "step": 3950
    },
    {
      "epoch": 0.792,
      "grad_norm": 3.0787513256073,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 1.6028,
      "step": 3960
    },
    {
      "epoch": 0.794,
      "grad_norm": 2.7373225688934326,
      "learning_rate": 1.4706666666666668e-05,
      "loss": 1.9008,
      "step": 3970
    },
    {
      "epoch": 0.796,
      "grad_norm": 2.4214577674865723,
      "learning_rate": 1.4693333333333336e-05,
      "loss": 1.7167,
      "step": 3980
    },
    {
      "epoch": 0.798,
      "grad_norm": 2.041374444961548,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 1.4943,
      "step": 3990
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.606212615966797,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 1.3553,
      "step": 4000
    },
    {
      "epoch": 0.802,
      "grad_norm": 2.954070806503296,
      "learning_rate": 1.4653333333333334e-05,
      "loss": 1.4877,
      "step": 4010
    },
    {
      "epoch": 0.804,
      "grad_norm": 3.004383087158203,
      "learning_rate": 1.464e-05,
      "loss": 1.7946,
      "step": 4020
    },
    {
      "epoch": 0.806,
      "grad_norm": 3.522920608520508,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 2.0838,
      "step": 4030
    },
    {
      "epoch": 0.808,
      "grad_norm": 3.3486328125,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 1.6744,
      "step": 4040
    },
    {
      "epoch": 0.81,
      "grad_norm": 2.5379741191864014,
      "learning_rate": 1.46e-05,
      "loss": 1.8326,
      "step": 4050
    },
    {
      "epoch": 0.812,
      "grad_norm": 3.047510862350464,
      "learning_rate": 1.4586666666666667e-05,
      "loss": 1.7076,
      "step": 4060
    },
    {
      "epoch": 0.814,
      "grad_norm": 3.8648698329925537,
      "learning_rate": 1.4573333333333335e-05,
      "loss": 1.8893,
      "step": 4070
    },
    {
      "epoch": 0.816,
      "grad_norm": 3.815075159072876,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 1.5461,
      "step": 4080
    },
    {
      "epoch": 0.818,
      "grad_norm": 2.813305616378784,
      "learning_rate": 1.4546666666666669e-05,
      "loss": 1.7493,
      "step": 4090
    },
    {
      "epoch": 0.82,
      "grad_norm": 3.138617753982544,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 1.5628,
      "step": 4100
    },
    {
      "epoch": 0.822,
      "grad_norm": 2.553459882736206,
      "learning_rate": 1.4520000000000002e-05,
      "loss": 1.5184,
      "step": 4110
    },
    {
      "epoch": 0.824,
      "grad_norm": 2.705669403076172,
      "learning_rate": 1.450666666666667e-05,
      "loss": 1.6303,
      "step": 4120
    },
    {
      "epoch": 0.826,
      "grad_norm": 2.769517660140991,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 1.6113,
      "step": 4130
    },
    {
      "epoch": 0.828,
      "grad_norm": 2.657243013381958,
      "learning_rate": 1.448e-05,
      "loss": 1.0802,
      "step": 4140
    },
    {
      "epoch": 0.83,
      "grad_norm": 3.8312957286834717,
      "learning_rate": 1.4466666666666668e-05,
      "loss": 1.4623,
      "step": 4150
    },
    {
      "epoch": 0.832,
      "grad_norm": 4.0277018547058105,
      "learning_rate": 1.4453333333333334e-05,
      "loss": 2.3353,
      "step": 4160
    },
    {
      "epoch": 0.834,
      "grad_norm": 4.384247303009033,
      "learning_rate": 1.444e-05,
      "loss": 2.1348,
      "step": 4170
    },
    {
      "epoch": 0.836,
      "grad_norm": 2.5481321811676025,
      "learning_rate": 1.4426666666666669e-05,
      "loss": 1.6179,
      "step": 4180
    },
    {
      "epoch": 0.838,
      "grad_norm": 2.6549580097198486,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 1.8067,
      "step": 4190
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.9624614715576172,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 1.3437,
      "step": 4200
    },
    {
      "epoch": 0.842,
      "grad_norm": 2.3615128993988037,
      "learning_rate": 1.4386666666666669e-05,
      "loss": 2.0566,
      "step": 4210
    },
    {
      "epoch": 0.844,
      "grad_norm": 3.3132212162017822,
      "learning_rate": 1.4373333333333335e-05,
      "loss": 1.8199,
      "step": 4220
    },
    {
      "epoch": 0.846,
      "grad_norm": 3.003392219543457,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 1.442,
      "step": 4230
    },
    {
      "epoch": 0.848,
      "grad_norm": 3.2384626865386963,
      "learning_rate": 1.434666666666667e-05,
      "loss": 1.382,
      "step": 4240
    },
    {
      "epoch": 0.85,
      "grad_norm": 3.565678834915161,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 2.0288,
      "step": 4250
    },
    {
      "epoch": 0.852,
      "grad_norm": 2.822176456451416,
      "learning_rate": 1.432e-05,
      "loss": 2.2049,
      "step": 4260
    },
    {
      "epoch": 0.854,
      "grad_norm": 3.149583578109741,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 1.5333,
      "step": 4270
    },
    {
      "epoch": 0.856,
      "grad_norm": 3.05531644821167,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 1.4325,
      "step": 4280
    },
    {
      "epoch": 0.858,
      "grad_norm": 3.050645351409912,
      "learning_rate": 1.428e-05,
      "loss": 1.7909,
      "step": 4290
    },
    {
      "epoch": 0.86,
      "grad_norm": 3.03255295753479,
      "learning_rate": 1.4266666666666668e-05,
      "loss": 1.7517,
      "step": 4300
    },
    {
      "epoch": 0.862,
      "grad_norm": 2.778778314590454,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 1.4191,
      "step": 4310
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.970902681350708,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 1.698,
      "step": 4320
    },
    {
      "epoch": 0.866,
      "grad_norm": 2.0975818634033203,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 1.6606,
      "step": 4330
    },
    {
      "epoch": 0.868,
      "grad_norm": 2.0549464225769043,
      "learning_rate": 1.4213333333333335e-05,
      "loss": 1.7156,
      "step": 4340
    },
    {
      "epoch": 0.87,
      "grad_norm": 3.4336159229278564,
      "learning_rate": 1.4200000000000001e-05,
      "loss": 1.5338,
      "step": 4350
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.8250699043273926,
      "learning_rate": 1.418666666666667e-05,
      "loss": 1.769,
      "step": 4360
    },
    {
      "epoch": 0.874,
      "grad_norm": 3.9952468872070312,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 1.8594,
      "step": 4370
    },
    {
      "epoch": 0.876,
      "grad_norm": 3.3157129287719727,
      "learning_rate": 1.416e-05,
      "loss": 1.3326,
      "step": 4380
    },
    {
      "epoch": 0.878,
      "grad_norm": 2.6742615699768066,
      "learning_rate": 1.4146666666666668e-05,
      "loss": 1.7203,
      "step": 4390
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.986299514770508,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 1.3336,
      "step": 4400
    },
    {
      "epoch": 0.882,
      "grad_norm": 3.838696002960205,
      "learning_rate": 1.412e-05,
      "loss": 1.7845,
      "step": 4410
    },
    {
      "epoch": 0.884,
      "grad_norm": 4.031759738922119,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 1.3968,
      "step": 4420
    },
    {
      "epoch": 0.886,
      "grad_norm": 3.2961575984954834,
      "learning_rate": 1.4093333333333334e-05,
      "loss": 1.7865,
      "step": 4430
    },
    {
      "epoch": 0.888,
      "grad_norm": 3.4511663913726807,
      "learning_rate": 1.408e-05,
      "loss": 1.7941,
      "step": 4440
    },
    {
      "epoch": 0.89,
      "grad_norm": 3.039200782775879,
      "learning_rate": 1.4066666666666669e-05,
      "loss": 1.9584,
      "step": 4450
    },
    {
      "epoch": 0.892,
      "grad_norm": 2.719149112701416,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 1.6885,
      "step": 4460
    },
    {
      "epoch": 0.894,
      "grad_norm": 3.1325056552886963,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 1.9857,
      "step": 4470
    },
    {
      "epoch": 0.896,
      "grad_norm": 2.4073455333709717,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 1.617,
      "step": 4480
    },
    {
      "epoch": 0.898,
      "grad_norm": 2.2899484634399414,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 1.5627,
      "step": 4490
    },
    {
      "epoch": 0.9,
      "grad_norm": 3.6413042545318604,
      "learning_rate": 1.4e-05,
      "loss": 1.5062,
      "step": 4500
    },
    {
      "epoch": 0.902,
      "grad_norm": 3.417926549911499,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 2.2429,
      "step": 4510
    },
    {
      "epoch": 0.904,
      "grad_norm": 2.616224527359009,
      "learning_rate": 1.3973333333333334e-05,
      "loss": 1.734,
      "step": 4520
    },
    {
      "epoch": 0.906,
      "grad_norm": 3.4512269496917725,
      "learning_rate": 1.396e-05,
      "loss": 2.2642,
      "step": 4530
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.128007173538208,
      "learning_rate": 1.3946666666666668e-05,
      "loss": 1.7842,
      "step": 4540
    },
    {
      "epoch": 0.91,
      "grad_norm": 2.5874085426330566,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 2.1997,
      "step": 4550
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.324045181274414,
      "learning_rate": 1.392e-05,
      "loss": 1.4576,
      "step": 4560
    },
    {
      "epoch": 0.914,
      "grad_norm": 2.855330228805542,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 1.993,
      "step": 4570
    },
    {
      "epoch": 0.916,
      "grad_norm": 3.393573522567749,
      "learning_rate": 1.3893333333333335e-05,
      "loss": 1.3962,
      "step": 4580
    },
    {
      "epoch": 0.918,
      "grad_norm": 2.9004647731781006,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 1.9269,
      "step": 4590
    },
    {
      "epoch": 0.92,
      "grad_norm": 2.5059006214141846,
      "learning_rate": 1.3866666666666669e-05,
      "loss": 1.8786,
      "step": 4600
    },
    {
      "epoch": 0.922,
      "grad_norm": 2.5478062629699707,
      "learning_rate": 1.3853333333333333e-05,
      "loss": 1.7962,
      "step": 4610
    },
    {
      "epoch": 0.924,
      "grad_norm": 2.8624000549316406,
      "learning_rate": 1.384e-05,
      "loss": 2.1775,
      "step": 4620
    },
    {
      "epoch": 0.926,
      "grad_norm": 2.0308825969696045,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 1.6769,
      "step": 4630
    },
    {
      "epoch": 0.928,
      "grad_norm": 2.1386959552764893,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 1.2614,
      "step": 4640
    },
    {
      "epoch": 0.93,
      "grad_norm": 3.747582197189331,
      "learning_rate": 1.38e-05,
      "loss": 2.1346,
      "step": 4650
    },
    {
      "epoch": 0.932,
      "grad_norm": 2.8875951766967773,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 1.4645,
      "step": 4660
    },
    {
      "epoch": 0.934,
      "grad_norm": 3.33172345161438,
      "learning_rate": 1.3773333333333334e-05,
      "loss": 1.5918,
      "step": 4670
    },
    {
      "epoch": 0.936,
      "grad_norm": 3.0318543910980225,
      "learning_rate": 1.376e-05,
      "loss": 2.1287,
      "step": 4680
    },
    {
      "epoch": 0.938,
      "grad_norm": 5.099884986877441,
      "learning_rate": 1.3746666666666668e-05,
      "loss": 1.6566,
      "step": 4690
    },
    {
      "epoch": 0.94,
      "grad_norm": 3.243316173553467,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 1.8879,
      "step": 4700
    },
    {
      "epoch": 0.942,
      "grad_norm": 2.324251174926758,
      "learning_rate": 1.3720000000000002e-05,
      "loss": 1.5338,
      "step": 4710
    },
    {
      "epoch": 0.944,
      "grad_norm": 4.159990310668945,
      "learning_rate": 1.3706666666666669e-05,
      "loss": 1.7487,
      "step": 4720
    },
    {
      "epoch": 0.946,
      "grad_norm": 2.6711840629577637,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 1.6711,
      "step": 4730
    },
    {
      "epoch": 0.948,
      "grad_norm": 2.698953151702881,
      "learning_rate": 1.3680000000000003e-05,
      "loss": 1.4834,
      "step": 4740
    },
    {
      "epoch": 0.95,
      "grad_norm": 2.8377203941345215,
      "learning_rate": 1.3666666666666667e-05,
      "loss": 2.2309,
      "step": 4750
    },
    {
      "epoch": 0.952,
      "grad_norm": 3.3895909786224365,
      "learning_rate": 1.3653333333333334e-05,
      "loss": 1.5604,
      "step": 4760
    },
    {
      "epoch": 0.954,
      "grad_norm": 5.336824417114258,
      "learning_rate": 1.3640000000000002e-05,
      "loss": 1.5532,
      "step": 4770
    },
    {
      "epoch": 0.956,
      "grad_norm": 2.4592292308807373,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 2.0141,
      "step": 4780
    },
    {
      "epoch": 0.958,
      "grad_norm": 2.3594706058502197,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 1.4123,
      "step": 4790
    },
    {
      "epoch": 0.96,
      "grad_norm": 2.747098445892334,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 1.5726,
      "step": 4800
    },
    {
      "epoch": 0.962,
      "grad_norm": 2.5666465759277344,
      "learning_rate": 1.3586666666666668e-05,
      "loss": 1.8584,
      "step": 4810
    },
    {
      "epoch": 0.964,
      "grad_norm": 3.0165488719940186,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 1.7177,
      "step": 4820
    },
    {
      "epoch": 0.966,
      "grad_norm": 3.478823184967041,
      "learning_rate": 1.3560000000000002e-05,
      "loss": 1.8866,
      "step": 4830
    },
    {
      "epoch": 0.968,
      "grad_norm": 3.192190170288086,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 2.1205,
      "step": 4840
    },
    {
      "epoch": 0.97,
      "grad_norm": 2.871403455734253,
      "learning_rate": 1.3533333333333333e-05,
      "loss": 1.588,
      "step": 4850
    },
    {
      "epoch": 0.972,
      "grad_norm": 2.592212200164795,
      "learning_rate": 1.3520000000000003e-05,
      "loss": 1.7725,
      "step": 4860
    },
    {
      "epoch": 0.974,
      "grad_norm": 2.0953571796417236,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 1.8453,
      "step": 4870
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.78324818611145,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 2.155,
      "step": 4880
    },
    {
      "epoch": 0.978,
      "grad_norm": 3.2698185443878174,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 1.7998,
      "step": 4890
    },
    {
      "epoch": 0.98,
      "grad_norm": 2.1027889251708984,
      "learning_rate": 1.3466666666666668e-05,
      "loss": 1.9724,
      "step": 4900
    },
    {
      "epoch": 0.982,
      "grad_norm": 2.500601291656494,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 1.8256,
      "step": 4910
    },
    {
      "epoch": 0.984,
      "grad_norm": 2.8237216472625732,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 1.6702,
      "step": 4920
    },
    {
      "epoch": 0.986,
      "grad_norm": 2.5909934043884277,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 1.6801,
      "step": 4930
    },
    {
      "epoch": 0.988,
      "grad_norm": 2.209049701690674,
      "learning_rate": 1.3413333333333334e-05,
      "loss": 1.3512,
      "step": 4940
    },
    {
      "epoch": 0.99,
      "grad_norm": 2.303617000579834,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 2.0066,
      "step": 4950
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.605459690093994,
      "learning_rate": 1.3386666666666668e-05,
      "loss": 2.2934,
      "step": 4960
    },
    {
      "epoch": 0.994,
      "grad_norm": 2.746534824371338,
      "learning_rate": 1.3373333333333333e-05,
      "loss": 1.9088,
      "step": 4970
    },
    {
      "epoch": 0.996,
      "grad_norm": 2.6838202476501465,
      "learning_rate": 1.3360000000000003e-05,
      "loss": 2.1578,
      "step": 4980
    },
    {
      "epoch": 0.998,
      "grad_norm": 3.851259469985962,
      "learning_rate": 1.3346666666666667e-05,
      "loss": 1.8893,
      "step": 4990
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.532305955886841,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.7609,
      "step": 5000
    }
  ],
  "logging_steps": 10,
  "max_steps": 15000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 676709007360000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
